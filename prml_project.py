# -*- coding: utf-8 -*-
"""PRML_MinorProject_B21EE070_B21CS095.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pGGwCmZOihaQc3OHGBM7MwmSOt91xjYj

Problem Statement:--
A company that sells some of the product, and you want to know how well the selling performance of the product. You have the data that we can analyze, but what kind of analysis can we do? Well, we can segment customers based on their buying behavior on the market. Your task is to classify the data into the possible types of customers which the retailer can encounter.
"""

# File is readed
import pandas as pd
data_frame=pd.read_excel("Online_Retail.xlsx")
data_frame.head()

from sklearn.impute import SimpleImputer

df=data_frame.copy()
imputer = SimpleImputer(strategy='mean')
df[['Quantity', 'UnitPrice']] = imputer.fit_transform(df[['Quantity', 'UnitPrice']])
print(df.isnull().sum())

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
df['Quantity'] = (df['Quantity'] - df['Quantity'].min()) / (df['Quantity'].max() - df['Quantity'].min())
df['UnitPrice'] = (df['UnitPrice'] - df['UnitPrice'].min()) / (df['UnitPrice'].max() - df['UnitPrice'].min())
df['TotalPrice'] = (df['Quantity'] *df['UnitPrice'])

#checking of missing values
data_frame.isnull().any()

#Creating some copies of the dataframe to use ahead 
df1=df2=df3=df4=df5=data_frame

# Preprocessing

# Dropping some unnecessary columns
# data_frame=data_frame.drop('InvoiceNo',axis='columns');

# Replacing missing values in description column with mode of the column
mode_of_description = data_frame['Description'].mode()

print("The mode for the column Description is ",mode_of_description[0])
mode_value=  mode_of_description[0];

# Replacing nan values with mode value

data_frame['Description'] = data_frame['Description'].fillna(mode_value)

# Again checking for missing values
print("Again checking missing values in Description column");
print(data_frame['Description'].isnull().any())

# Replacing missing value in customer Id with mode
mode_val2 = data_frame['CustomerID'].mode()[0];
print("The mode value for CustomerID is ",mode_val2);
data_frame['CustomerID']= data_frame['CustomerID'].fillna(mode_val2)

print("Again checking missing values in CustomerID column");
print(data_frame['CustomerID'].isnull().any())

# Printing the preprocessed dataframe
print("The preprocessed dataframe is ")
print(data_frame);

# Data Visualisation and analysis
import pandas as pd

# Unique country names
country_names = pd.unique(data_frame['Country']);

# Unique product names
product_description = pd.unique(data_frame["Description"]);

key_values = country_names ;
count_dictionary={};
val_values = [];

# Finding a dictionary which holds a count of every product's sale according to every country
for j in country_names:
  dict_temp={};
  temp_dataset = data_frame[data_frame['Country']==j];
  for i in temp_dataset["Description"].unique():
           dict_temp[i]=0
  for i in temp_dataset["Description"].keys():
           dict_temp[temp_dataset["Description"][i]]+=temp_dataset["Quantity"][i]
  count_dictionary[j]=dict_temp
print(count_dictionary)

# Sorting the dictionary of counts of every product to get some best selling products in a country
for i in count_dictionary:
  temp_dict= count_dictionary[i];
  sorted_values = sorted(temp_dict.values());
  sorted_dict = {} ;
  for sort in sorted_values:
    for keys in temp_dict.keys():
      if (temp_dict[keys] == sort):
        sorted_dict[keys ]= temp_dict[keys];
        break;
  count_dictionary[i]= sorted_dict ;
print(count_dictionary);

#Plotting the results to visualise the data 
from pandas.core.algorithms import value_counts_arraylike
import matplotlib.pyplot as plt

for i in count_dictionary:
  key_array=[];
  value_array=[];
  value_array_sorted_ten_top_values=[];
  key_array= list(count_dictionary[i].keys());
  key_array_sorted_top_ten=[];
  value_array= list(count_dictionary[i].values());
  for d in range(0,10):
    if(len(value_array)-1-d>=0):
      value_array_sorted_ten_top_values.append(value_array[len(value_array)-1-d]);
    if(len(key_array)-1-d>=0):
      key_array_sorted_top_ten.append(key_array[len(key_array)-1-d]);
  
  fig = plt.figure(figsize = (32, 13));
  plt.bar(key_array_sorted_top_ten, value_array_sorted_ten_top_values, color ='maroon',
        width = 0.3)
  print("For ",i);
  plt.xlabel("product names")
  plt.ylabel("Quantity of product buyed")
  plt.title("visualisation of products sale within a country")
  plt.show()

# Data analysis by sns pie plot
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_style('darkgrid')

country_counts = df['Country'].value_counts()
plt.pie(country_counts, labels=country_counts.index, autopct='%1.1f%%')

# Analysing the correlation between different features
corr_matrix = df.corr()
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)

# Analysing and visualizing the monthly sales 
monthly_sales = df.groupby(pd.Grouper(key='InvoiceDate', freq='M'))['TotalPrice'].sum().reset_index()
sns.lineplot(x='InvoiceDate', y='TotalPrice', data=monthly_sales)

import pandas as pd

# Spliting and separating the dataset by country

dfs_by_country = {}

dfs_by_country = {}  
for country in pd.unique(df3['Country']):
    mask = df3['Country'] == country  
    dfs_by_country[country] = df3.loc[mask]  

# Function to update total quantity of sold product in a country
def total_quantity_sold(df_country):
  quantity_by_stockcode = df_country.groupby(['StockCode'])['Quantity'].sum().sort_values(ascending=False);
  return quantity_by_stockcode;

# Function to update total revenue in a country according to different products
def total_revenue(df_country):
  revenue_by_stockcode = df_country.groupby(['StockCode'])[['Quantity', 'UnitPrice']].apply(lambda x: (x['Quantity']*x['UnitPrice']).sum()).sort_values(ascending=False)
  return revenue_by_stockcode;

# Doing analysis based on StockCode for each country

for country, df_country in dfs_by_country.items():
    # Calculating total revenue by stock
    revenue_by_stockcode = total_revenue(df_country);

    # Calculate total quantity sold by StockCode
    quantity_by_stockcode = total_quantity_sold(df_country);

    # Print top 10 revenue and quantity StockCodes for each country
    print('Country:', country)
    print('Top 10 StockCodes by Revenue:')
    print(revenue_by_stockcode.head(10))
    print('Top 10 StockCodes by Quantity:')
    print(quantity_by_stockcode.head(10))
    print()

df6=df7=data_frame.copy()
df6 = df6.dropna().reset_index(drop=True)

! pip install wordcloud

# Plotting word cloud to visualise product names frequency in Description column
from wordcloud import WordCloud
# print(df6['Description'])
array=[]
for i in range(0,len(df6['Description'])):
  array.append(df6['Description'][i]);
array.remove(array[420391])

Desc = ' '.join(array)
stop_word = ['red', 'pink', 'blue', 'of', 'white', 'small', 't', 'and',
             'in', 'set']
wc = WordCloud(collocations=False,
               stopwords=stop_word,
               background_color='white').generate(Desc.lower())
plt.imshow(wc, interpolation='bilinear');
plt.axis('off');

# Calculate RFM scores for each customer
rfm = df.groupby('CustomerID').agg({'InvoiceDate': lambda x: (df['InvoiceDate'].max() - x.max()).days,
                                    'InvoiceNo': 'count',
                                    'TotalPrice': 'sum'})

# Rename the columns
rfm.rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency', 'TotalPrice': 'Monetary'}, inplace=True)

# Create labels for the Recency and Frequency variables
r_labels = range(4, 0, -1)
f_labels = range(1, 5)

# Assign the labels to the quartiles of the Recency and Frequency variables
rfm['R'] = pd.qcut(rfm['Recency'], q=4, labels=r_labels)
rfm['F'] = pd.qcut(rfm['Frequency'], q=4, labels=f_labels)

# Assign the labels to the quartiles of the Monetary variable
m_labels = range(1, 5)
rfm['M'] = pd.qcut(rfm['Monetary'], q=4, labels=m_labels)

# Combine the RFM labels into a single string
rfm['RFM_Segment'] = rfm['R'].astype(str) + rfm['F'].astype(str) + rfm['M'].astype(str)

# Calculate the RFM score as the sum of the quartile labels
rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1)

# Display the RFM analysis results
print(rfm.head())

rfm.keys()

# Select the RFM scores from the preprocessed data
rfm_data = rfm[['Recency', 'Frequency', 'Monetary']].drop_duplicates()

# Standardize the data using the z-score transformation
from scipy.stats import zscore
rfm_data_zscore = rfm_data.apply(zscore)

# Perform K-Means clustering with k=4
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, random_state=0).fit(rfm_data_zscore)

# Assign the cluster labels to the original dataset
rfm_data['Cluster'] = kmeans.labels_

# Visualize the clusters using scatter plots
import seaborn as sns
sns.scatterplot(x='Recency', y='Frequency', hue='Cluster', data=rfm_data)

# Selecting  the features to be used in clustering
cluster_data = rfm[['Recency', 'Frequency', 'Monetary']]

# Standardize the data using the z-score transformation
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
cluster_data_scaled = scaler.fit_transform(cluster_data)

# Perform K-Means clustering with different values of k
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

k_values = range(2, 11)
sse_values = []
silhouette_scores = []
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(cluster_data_scaled)
    sse_values.append(kmeans.inertia_)
    if k > 1:
        silhouette_scores.append(silhouette_score(cluster_data_scaled, kmeans.labels_))

# Plot the elbow curve
fig, ax = plt.subplots(1, 2, figsize=(12, 4))
ax[0].plot(k_values, sse_values, 'bx-')
ax[0].set_xlabel('Number of clusters (k)')
ax[0].set_ylabel('Sum of squared errors (SSE)')
ax[0].set_title('Elbow Method')

# Plot the silhouette scores
ax[1].plot(k_values, silhouette_scores, 'bx-')
ax[1].set_xlabel('Number of clusters (k)')
ax[1].set_ylabel('Silhouette Score')
ax[1].set_title('Silhouette Score Method')

plt.show()

# Perform KMeans clustering on the data
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42,n_init=10)
kmeans.fit(cluster_data_scaled)

# Reducing the dimensionality of the data using PCA to use ahead
pca = PCA(n_components=3)
df_pca = pca.fit_transform(cluster_data_scaled)

# Ploting the clusters in 2D space

plt.scatter(df_pca[:, 0], df_pca[:, 1], c=kmeans.labels_)
plt.title('Customer Segmentation')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# Applying K-means clustering on the preprocessed dataset on different set of features- Quantity and unit price

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Selecting the needed columns for clustering
X = df1[['Quantity', 'UnitPrice']]

# Scale the data to have zero mean and unit variance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit K-means clustering algorithm to the data
kmeans = KMeans(n_clusters=3, random_state=0,n_init=10)
kmeans.fit(X_scaled)

# Add the cluster labels to the original data
df1['cluster'] = kmeans.labels_

# Analyze the clusters
cluster_summary = df1.groupby('cluster')[['Quantity', 'UnitPrice']].mean()

# Print the cluster summary
print("The cluster summary is as follows--")
print(cluster_summary)

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
df2=data_frame.copy()
# Preprocess the data for customer segmentation
df2 = df2.groupby(['CustomerID']).agg({
    'InvoiceNo': 'nunique',
    'Quantity': 'sum',
})

df2 = df2.dropna()

# Scaling the data for KMeans clustering
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df2)

# Find optimal number of clusters using the elbow method
def optimal_clusters(df_scaled):
  wcss = []
  for i in range(1, 11):
      kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
      kmeans.fit(df_scaled)
      wcss.append(kmeans.inertia_)
  plt.plot(range(1, 11), wcss)
  plt.title('Elbow Method')
  plt.xlabel('Number of Clusters')
  plt.ylabel('WCSS')
  plt.show()

optimal_clusters(df_scaled);

# Perform KMeans clustering on the data
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42,n_init=10)
kmeans.fit(df_scaled)

# Reducing the dimensionality of the data using PCA to use ahead
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_scaled)

# Ploting the clusters in 2D space

plt.scatter(df_pca[:, 0], df_pca[:, 1], c=kmeans.labels_)
plt.title('Customer Segmentation')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

# Heirarchial Clustering 

# Selecting the features to be used in clustering
cluster_data = rfm[['Recency', 'Frequency', 'Monetary']]

# Standardizeing the data using the z-score transformation
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
cluster_data_scaled = scaler.fit_transform(cluster_data)

# Now Performing hierarchical clustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

linked = linkage(cluster_data_scaled, method='ward')
fig = plt.figure(figsize=(10, 7))
dendrogram(linked)

# Ploting dendogram
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Distance')
plt.show()

# Agglomerative Clustering

# Select the features to be used in clustering
cluster_data = rfm[['Recency', 'Frequency', 'Monetary']]

# Standardize the data using the z-score transformation
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
cluster_data_scaled = scaler.fit_transform(cluster_data)

# Perform agglomerative clustering
from sklearn.cluster import AgglomerativeClustering

agglo = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
agglo_labels = agglo.fit_predict(cluster_data_scaled)

# Plot the clusters
plt.scatter(cluster_data_scaled[:, 0], cluster_data_scaled[:, 1], c=agglo_labels)
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.title('Agglomerative Clustering')
plt.show()

# Db scan clustering

# Select the features to be used in clustering
cluster_data = rfm[['Recency', 'Frequency', 'Monetary']]

# Standardize the data using the z-score transformation
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
cluster_data_scaled = scaler.fit_transform(cluster_data)

# Perform DBSCAN clustering
from sklearn.cluster import DBSCAN
import numpy as np

dbscan = DBSCAN(eps=1.2, min_samples=5)
dbscan_labels = dbscan.fit_predict(cluster_data_scaled)

# Print the number of clusters and noise points
num_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
num_noise_points = list(dbscan_labels).count(-1)
print('Number of clusters:', num_clusters)
print('Number of noise points:', num_noise_points)

# Plot the clusters
plt.scatter(cluster_data_scaled[:, 0], cluster_data_scaled[:, 1], c=dbscan_labels)
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.title('DBSCAN Clustering')
plt.show()